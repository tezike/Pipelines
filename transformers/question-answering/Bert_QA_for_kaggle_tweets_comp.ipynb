{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_QA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-HlheOolrttZ"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOwWS33UWIFNZgpzSmYzCsC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e581ced5946848e8a259b1d01e2d72ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_69a5b26a1afc45df89cf5369d3457a59",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9d89c61b3f8644b4867361f8a2b79773",
              "IPY_MODEL_62a8e959b9584d94978e04911b66460a"
            ]
          }
        },
        "69a5b26a1afc45df89cf5369d3457a59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d89c61b3f8644b4867361f8a2b79773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_760969eb90de4d11a3c40381080caa44",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e049d5d20eed4a47b95332eef7ce5aac"
          }
        },
        "62a8e959b9584d94978e04911b66460a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8036d4d584cd49ceb82bc75927b1595b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 361/361 [00:00&lt;00:00, 5.27kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9030cf78f79c4c76aadc28b4bc886633"
          }
        },
        "760969eb90de4d11a3c40381080caa44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e049d5d20eed4a47b95332eef7ce5aac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8036d4d584cd49ceb82bc75927b1595b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9030cf78f79c4c76aadc28b4bc886633": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d3d77520bef449c39c862005a69375c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2536c3fdc49e4a08959fa7dc47018d91",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_430c9c649552431181b319e6a002d1ee",
              "IPY_MODEL_18ebad46b2c04d47900840b9e0adee27"
            ]
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqsD3WlAwwO0",
        "colab_type": "text"
      },
      "source": [
        "## Kaggle Tweet Sentiment Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltLWRlKrxNoE",
        "colab_type": "text"
      },
      "source": [
        "This notebook is built on the awesome tutorial made by Abhishek [here](https://www.youtube.com/watch?v=XaQ0CBlQ4cY). Thanks man!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHQijHBhxhL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQNK70EuxrRr",
        "colab_type": "text"
      },
      "source": [
        "## Colab_setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiTyipOvcPQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nipZBIQ2amuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_path(path):\n",
        "    if not os.path.isdir(path):\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "    return path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdKS3nNT0IKp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "39bc0426-ad77-431c-8ac8-922869a46bce"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root_dir = Path('/content/drive/My Drive')\n",
        "base_path = create_path(root_dir/'Kaggle_Twitter_Bert')\n",
        "base_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/My Drive/Kaggle_Twitter_Bert')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIXgl6523wqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colab_path = Path('/content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BTdMY8caahd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = create_path(base_path/'dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw59N97DV63P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = create_path(base_path/'models')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIp4JAvv3rGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_path = (create_path(colab_path/'input/bert_uncased'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcRo2fCS1a49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############join all above to below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usjQeC0LGK1A",
        "colab_type": "text"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSj-x_eGJnTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"'https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/16295/1099992/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587716090&Signature=nKyFPV0iiqPX3alWIj8DDbm2kpW%2Bb2jWgFU06dHIIFFC5%2FIif380Zs32%2F7FM4pMD3xNgGGsYcnZVhxbyn1T7FQfsUZ%2B5SeEIHn%2BEeOVAOrz1GVJs9fzVQ%2B%2FTHGfhvgJaOzejXcwONLyUAH2%2FkRnRvrq%2Bx6ghUwDLHeKnjmQpVYGztyFlX3eGNHBmKVvjmtGVhnZp28jx7lQ0qoVOSE2%2BoGYCsByw%2BK9Rqh8SNQW8NEt6TGQ%2B0HOCJB8tWwio96qZ%2Byyq3tVqaZLD8%2BdFi14MotnsnpvuiE84AmZTdtFa%2FnH4s6aV%2BCtBS%2FPYYrmM9nUmrTcZeGMEblMm17C93NzSGQ%3D%3D&response-content-disposition=attachment%3B+filename%3Dtweet-sentiment-extraction.zip'\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7NoYjpfGN_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uncomment this if running for first time\n",
        "\n",
        "# os.chdir(data_path)\n",
        "# !wget -q {str(url)} -O temp.zip && unzip -q temp.zip && rm 'temp.zip'\n",
        "# os.chdir(colab_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HlheOolrttZ",
        "colab_type": "text"
      },
      "source": [
        "##Download Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd6sEjSqrxYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IB09BCkrwZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(bert_path)\n",
        "!wget -q {str(url)} -O 'vocab.txt'\n",
        "os.chdir(colab_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StWgJac61ouP",
        "colab_type": "text"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRhI9sD7LKGq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "7c2cfe67-7d63-46fe-f7ad-890fc383dbe9"
      },
      "source": [
        "!pip -q install transformers\n",
        "!pip -q install tokenizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 573kB 4.9MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.7MB 22.1MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0MB 50.8MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 48.6MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLfoy4Kt1cSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import transformers\n",
        "import tokenizers\n",
        "\n",
        "class Config():\n",
        "    def __init__(self):\n",
        "        self.MAX_LEN = 512\n",
        "        self.SAVE_MODEL_PATH = str(model_path/'finetuned-bert.pth')\n",
        "        self.DATA_PATH = str(data_path/'train_fold.csv')\n",
        "        self.BERT_PATH = str(bert_path/'finetuned-bert.pth')\n",
        "        self.VOCAB_PATH = str(bert_path/'vocab.txt')\n",
        "        self.TRAIN_BATCH_SIZE = 8\n",
        "        self.VALID_BATCH_SIZE = 4\n",
        "        self.NUM_EPOCHS = 10\n",
        "        self.MODEL_NAME = 'bert-base-uncased'\n",
        "        # self.TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
        "        #     pretrained_model_name_or_path=self.MODEL_NAME,\n",
        "        #     do_lower_case=True,\n",
        "        #     force_download = True,\n",
        "        # )\n",
        "        self.TOKENIZER = tokenizers.BertWordPieceTokenizer(\n",
        "            self.VOCAB_PATH, \n",
        "            lowercase=True\n",
        "        )\n",
        "\n",
        "config = Config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8Gur65yx5sB",
        "colab_type": "text"
      },
      "source": [
        "##Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8IfXU211Q-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "# Handling this problem as a question answering problem means that we take in \n",
        "# question <sentiment> and we expect an answer <extracted_text> from the original tweet\n",
        "def process_data(tweet, extracted_text, sentiment, tokenizer, max_len):\n",
        "    len_et = len(extracted_text)\n",
        "\n",
        "    # we have to set the ids at where the extracted text starts and ends in the original tweet text\n",
        "    idx_start, idx_end = 0, 0\n",
        "\n",
        "    # check where the exctracted text starts in the main tweet and then extract the id\n",
        "    # NB: This should be done before tokenization so as account for broken words or incompleted extractions\n",
        "    for idx in (i for i, word in enumerate(tweet) if word == extracted_text[0]):\n",
        "        # check if the complete extracted word is a subset of the tweet using \n",
        "        # the lenght of the extrcated text\n",
        "        if tweet[idx:idx+len_et] == extracted_text:\n",
        "            idx_start = idx\n",
        "            idx_end = idx + len_et - 1\n",
        "            break\n",
        "\n",
        "\n",
        "    # we are trying to use n-hot encoding to indicate where the tokens from the \n",
        "    # extracted text match the original tweet\n",
        "    n_hot_enc = [0] * len(tweet)\n",
        "\n",
        "    # sanity check\n",
        "    if idx_start!=None and idx_end!=None:\n",
        "        for ind in range(idx_start, idx_end+1):\n",
        "            n_hot_enc[ind] = 1\n",
        "\n",
        "    # now we can tokenize the tweet. Remember that Bert adds [CLS] and [SEP] tokens to the sentence\n",
        "    tweet_token = tokenizer.encode(tweet)\n",
        "    # we want to remove the [CLS] and [SEP] tokens at the start and end of the sentence\n",
        "    token_ids = tweet_token.ids[1:-1]\n",
        "    tweet_offsets = tweet_token.offsets[1:-1]\n",
        "\n",
        "    targets = []\n",
        "    for i, (start_off, end_off) in enumerate(tweet_offsets):\n",
        "        # get the extracted text ids from the original tweet text\n",
        "        if sum(token_ids[start_off:end_off]) > 0:\n",
        "            targets.append(i)\n",
        "    \n",
        "    # get the indexes where the extracted text(answers) started from and ended\n",
        "    target_start = targets[0]\n",
        "    target_end = targets[-1]\n",
        "\n",
        "\n",
        "    # extract the tokens for the questions\n",
        "    sentiment_dict = {\n",
        "        'neutral' : config.TOKENIZER.encode('Neutral').ids[1],\n",
        "        'positive' : config.TOKENIZER.encode('Positive').ids[1],\n",
        "        'negative' : config.TOKENIZER.encode('Negative').ids[1],\n",
        "    }\n",
        "\n",
        "    # the [CLS] and [SEP] token ids\n",
        "    CLS = tokenizer.encode('[CLS]').ids[1]\n",
        "    SEP = tokenizer.encode('[SEP]').ids[1]\n",
        "\n",
        "    # question answering systems for transformer(Bert) arch take the form\n",
        "    # [cls]<question_tokens>[sep]<answer_token>[sep]\n",
        "    # then the token_type_ids are\n",
        "    # 00011 where the 0 stops at the middle [sep] token shown above \n",
        "    qa_inputs = [CLS] + [sentiment_dict[sentiment]] + [SEP] + token_ids + [SEP]\n",
        "    token_ids = [0,0,0] + ([1] * len(token_ids + [0]))\n",
        "    tweet_offsets = [(0, 0)] * 3 + tweet_offsets + [(0, 0)]\n",
        "    mask = [1] * len(token_ids)\n",
        "    #offset the targets considering the [CLS] + sentiment_dict[sentiment] + [SEP] tokens\n",
        "    target_start += 3\n",
        "    target_end += 3\n",
        "\n",
        "    # now let's handle padding\n",
        "    padding_sz = max_len - len(token_ids)\n",
        "    \n",
        "    if padding_sz>0:\n",
        "        # bert uses 0 padding\n",
        "        qa_inputs = qa_inputs + ([0] * padding_sz)\n",
        "        token_ids = token_ids + ([0] * padding_sz)\n",
        "        tweet_offsets = tweet_offsets + ([(0,0)] * padding_sz)\n",
        "        mask = mask + ([0] * padding_sz)\n",
        "\n",
        "    \n",
        "\n",
        "    return {\n",
        "        'input_ids': torch.tensor(qa_inputs).long(), \n",
        "        'token_ids': torch.tensor(token_ids).long(),\n",
        "        'mask': torch.tensor(mask).long(),\n",
        "        'target_start': torch.tensor(target_start).long(),\n",
        "        'target_end': torch.tensor(target_end).long(),\n",
        "        'tweet_offsets': torch.tensor(tweet_offsets).long(),\n",
        "        'tweet': tweet,\n",
        "        'extracted_text': extracted_text,\n",
        "        'sentiment': sentiment,\n",
        "    }\n",
        "\n",
        "\n",
        "class TwitterDataset():\n",
        "    def __init__(self, tweet, extracted_text, sentiment, tokenizer, max_len):\n",
        "        (self.tweet, self.extracted_text, self.sentiment, self.tokenizer, self.max_len) = \\\n",
        "        (tweet, extracted_text, sentiment, tokenizer, max_len)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweet)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sanity check\n",
        "        tweet = str(self.tweet[idx])\n",
        "        tweet = ' '.join(tweet.split())\n",
        "        extracted_text = str(self.extracted_text[idx])\n",
        "        extracted_text = ' '.join(extracted_text.split())\n",
        "\n",
        "        return process_data(tweet, extracted_text, self.sentiment[idx], \n",
        "                     self.tokenizer, self.max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS3BYnlpyCjH",
        "colab_type": "text"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdhAG8nFz2cg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import transformers\n",
        "\n",
        "# make a pythorch model\n",
        "class Bert(nn.Module):\n",
        "    def __init__(self, xtra_config):\n",
        "        super(Bert, self).__init__()\n",
        "        # load a pretrained bert model arch\n",
        "        self.bert = transformers.BertModel.from_pretrained(config.MODEL_NAME, config=xtra_config)\n",
        "        #  dropout should be applied\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        # a classifier head should be placed. It will give us the strat and end \n",
        "        # of the extracted token from the orig\n",
        "        self.head = nn.Linear(768*2, 2) #double because of concatenation    \n",
        "\n",
        "        # use a noraml init or any of your choice\n",
        "        torch.nn.init.normal_(self.head.weight, std=0.02)\n",
        "\n",
        "    def forward(self, stoi, mask, token_type_ids):\n",
        "        # according to the docs, it is better to use the hidden_state output \n",
        "        # which is gotten by setting the config.output_hidden_states = True\n",
        "        # 13 * (bs, seq_len, 768) for hidden_states\n",
        "        _, _, hidden_states = self.bert(input_ids=stoi, \n",
        "                                        attention_mask=mask, \n",
        "                                        token_type_ids=token_type_ids)\n",
        "        \n",
        "        # by default, bert has 13 hidden states. it is advised here \n",
        "        # https://bert-as-service.readthedocs.io/en/latest/section/faq.html#why-not-the-last-hidden-layer-why-second-to-last \n",
        "        # to use the last 2 instead of just the last for tasks other than MLM or NSP\n",
        "        # since we have two outputs for start and stop, we'll have two hidden states\n",
        "        h0, h1 = hidden_states[-2], hidden_states[-1] \n",
        "\n",
        "        # (bs, seq_len, 768*2)\n",
        "        logits = torch.cat((h0, h1), dim=-1)\n",
        "\n",
        "        # pass logits into dropout\n",
        "        logits = self.drop(logits)\n",
        "\n",
        "        # pass into classifier head so (bs, seq_len, 2)\n",
        "        logits = self.head(logits)\n",
        "\n",
        "        # we can now split this output to get our start and end for the targets\n",
        "        # split takes the chunk size for the specified dimension. So take a chunk size of 1 at the last dim\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        return start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkKFWxDbyXok",
        "colab_type": "text"
      },
      "source": [
        "##Train_utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZgniTJefE54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import notebook\n",
        "import numpy as np\n",
        "\n",
        "def loss_func(pred_target_start, pred_target_end, actual_start, actual_end):\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    start_loss = loss(pred_target_start, actual_start)\n",
        "    end_loss = loss(pred_target_end, actual_end)\n",
        "    total_loss = start_loss + end_loss\n",
        "    return total_loss\n",
        "\n",
        "def train_fn(dataloader, model, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "\n",
        "    # we use averagemeter to find the average of the losses\n",
        "    jaccard_all = AverageMeter()\n",
        "    loss_all = AverageMeter()\n",
        "    tk0 = notebook.tqdm(dataloader, total=len(dataloader))\n",
        "\n",
        "    for i, data in enumerate(tk0):\n",
        "\n",
        "        input_ids = data['input_ids']\n",
        "        token_ids = data['token_ids']\n",
        "        mask = data['mask']\n",
        "        target_start = data['target_start']\n",
        "        target_end = data['target_end']\n",
        "        tweet_offsets = data['tweet_offsets']\n",
        "        tweet = data['tweet']\n",
        "        extracted_text = data['extracted_text']\n",
        "        sentiment = data['sentiment']\n",
        "\n",
        "        # put em on the device\n",
        "        input_ids = input_ids.to(device).long()\n",
        "        token_ids = token_ids.to(device).long()\n",
        "        mask = mask.to(device).long()\n",
        "        target_start = target_start.to(device).long()\n",
        "        target_end = target_end.to(device).long()\n",
        "        tweet_offsets = tweet_offsets.to(device).long()\n",
        "\n",
        "        # zero grad in model if any\n",
        "        model.zero_grad()\n",
        "\n",
        "        # push it into the model\n",
        "        pred_start, pred_end = model(\n",
        "            stoi = input_ids, \n",
        "            mask = mask, \n",
        "            token_type_ids = token_ids\n",
        "        )\n",
        "\n",
        "        # calc loss\n",
        "        loss = loss_func(pred_start, pred_end, target_start, target_end)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # calculate jaccard\n",
        "        # we have to convert the predicted logits to probability values for each targ\n",
        "        pred_start_probs = torch.softmax(pred_start, dim=1).cpu().detach().numpy()\n",
        "        pred_end_probs = torch.softmax(pred_end, dim=1).cpu().detach().numpy()\n",
        "\n",
        "        jaccards = []\n",
        "\n",
        "        for idx, tweet_ in enumerate(tweet):\n",
        "            pred_start_prob = np.argmax(np.array(pred_start_probs)[idx, :])\n",
        "            pred_end_prob = np.argmax(np.array(pred_end_probs)[idx, :])\n",
        "\n",
        "            jaccard, _ = calculate_jaccard(tweet_, extracted_text[idx], sentiment[idx], \n",
        "                                  tweet_offsets[idx], pred_start_prob, pred_end_prob)\n",
        "            jaccards.append(jaccard)\n",
        "\n",
        "        # use averagemeter to update\n",
        "        jaccard_all.update(np.mean(jaccards), input_ids.size(0)) #bs\n",
        "        loss_all.update(loss.item(), input_ids.size(0)) #bs\n",
        "\n",
        "        # show avg loss after every iter\n",
        "        tk0.set_postfix(loss=loss_all.avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsdOzAi8ePSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_fn(dataloader, model, device):\n",
        "    model.eval()\n",
        "\n",
        "    # we use averagemeter to find the average of the losses\n",
        "    jaccard_all = AverageMeter()\n",
        "    loss_all = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tk0 = notebook.tqdm(dataloader, total=len(dataloader))\n",
        "        for i, data in enumerate(tk0):\n",
        "\n",
        "            input_ids = data['input_ids']\n",
        "            token_ids = data['token_ids']\n",
        "            mask = data['mask']\n",
        "            target_start = data['target_start']\n",
        "            target_end = data['target_end']\n",
        "            tweet_offsets = data['tweet_offsets']\n",
        "            tweet = data['tweet']\n",
        "            extracted_text = data['extracted_text']\n",
        "            sentiment = data['sentiment']\n",
        "\n",
        "            # put em on the device\n",
        "            input_ids = input_ids.to(device).long()\n",
        "            token_ids = token_ids.to(device).long()\n",
        "            mask = mask.to(device).long()\n",
        "            target_start = target_start.to(device).long()\n",
        "            target_end = target_end.to(device).long()\n",
        "            tweet_offsets = tweet_offsets.to(device).long()\n",
        "\n",
        "            # push it into the model\n",
        "            pred_start, pred_end = model(\n",
        "                stoi = input_ids, \n",
        "                mask = mask, \n",
        "                token_type_ids = token_ids\n",
        "            )\n",
        "\n",
        "            # calc loss\n",
        "            loss = loss_func(pred_start, pred_end, target_start, target_end)\n",
        "\n",
        "            # calculate jaccard\n",
        "            # we have to convert the predicted logits to probability values for each targ\n",
        "            pred_start_probs = torch.softmax(pred_start, dim=1).cpu().detach().numpy()\n",
        "            pred_end_probs = torch.softmax(pred_end, dim=1).cpu().detach().numpy()\n",
        "\n",
        "            jaccards = []\n",
        "            for idx, tweet_ in enumerate(tweet):\n",
        "                pred_start_prob = np.argmax(pred_start_probs[idx, :])\n",
        "                pred_end_prob = np.argmax(pred_end_probs[idx, :])\n",
        "\n",
        "                jaccard, _ = calculate_jaccard(tweet_, extracted_text[idx], sentiment[idx], \n",
        "                                    tweet_offsets[idx], pred_start_prob, pred_end_prob)\n",
        "                jaccards.append(jaccard)\n",
        "\n",
        "            # use averagemeter to update\n",
        "            jaccard_all.update(np.mean(jaccards), input_ids.size(0)) #bs\n",
        "            loss_all.update(loss.item(), input_ids.size(0)) #bs\n",
        "\n",
        "            # show avg loss after every iter\n",
        "            tk0.set_postfix(loss=loss_all.avg)\n",
        "    \n",
        "    print(f'Jaccard = {jaccard_all.avg}')\n",
        "    return jaccard_all.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYXermRcyHen",
        "colab_type": "text"
      },
      "source": [
        "##Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG8CGb2sYOj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter():\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def jaccard_score(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-76NgLDvkrpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_jaccard(orig_tweet, extracted_text, sentiment, offsets, pred_start, pred_end):\n",
        "\n",
        "    # instantiate the predictions we expect\n",
        "    pred_extract =  ''\n",
        "\n",
        "    # we've established that the extracted text for neutral tweets is usually the \n",
        "    # entire tweet. We also know that for tweets with at most 2 charcters, \n",
        "    # the extract will be from those chars \n",
        "    if sentiment == 'neutral' and len(orig_tweet.split()) < 2:\n",
        "        pred_extract = orig_tweet\n",
        "    else:\n",
        "        # sanity\n",
        "        if pred_start > pred_end:\n",
        "            pred_end = pred_start #zero things out\n",
        "\n",
        "        for i in range(pred_start, pred_end+1):\n",
        "            # use the offsets to get the predicted extracted text\n",
        "            pred_extract += orig_tweet[offsets[i][0]:offsets[i][1]]\n",
        "\n",
        "            # add spaces between the extracts for clarity \n",
        "            if i+1 < len(offsets) and offsets[i][1] < offsets[i+1][0]:\n",
        "                pred_extract = ' '\n",
        "\n",
        "    jaccard = jaccard_score(pred_extract, extracted_text)\n",
        "\n",
        "    return jaccard, pred_extract"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujwMIqxqczyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.mode = mode\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "        if self.mode == \"min\":\n",
        "            self.val_score = np.Inf\n",
        "        else:\n",
        "            self.val_score = -np.Inf\n",
        "\n",
        "    def __call__(self, epoch_score, model, model_path):\n",
        "\n",
        "        if self.mode == \"min\":\n",
        "            score = -1.0 * epoch_score\n",
        "        else:\n",
        "            score = np.copy(epoch_score)\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(epoch_score, model, model_path)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(epoch_score, model, model_path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, epoch_score, model, model_path):\n",
        "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
        "            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "        self.val_score = epoch_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blRAauiWicBr",
        "colab_type": "text"
      },
      "source": [
        "##Stratify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60nc6SWgikKi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "514c696b-1d50-402d-cedb-440d1851cf0c"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "df = pd.read_csv(data_path/'train.csv').dropna().reset_index(drop=True)\n",
        "df['kfold'] = -1\n",
        "\n",
        "# sample the full data to add shuffling\n",
        "df = df.sample(frac=1.)\n",
        "\n",
        "# use 5 folds\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "for fold, (train, valid) in enumerate(kf.split(X=df, y=df.sentiment.values)):\n",
        "    print(len(train), len(valid), fold)\n",
        "    df.loc[valid, 'kfold'] = fold\n",
        "\n",
        "df.to_csv(data_path/'train_fold.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21984 5496 0\n",
            "21984 5496 1\n",
            "21984 5496 2\n",
            "21984 5496 3\n",
            "21984 5496 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdMkarywyLnQ",
        "colab_type": "text"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAXw1EskNONQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "e581ced5946848e8a259b1d01e2d72ca",
            "69a5b26a1afc45df89cf5369d3457a59",
            "9d89c61b3f8644b4867361f8a2b79773",
            "62a8e959b9584d94978e04911b66460a",
            "760969eb90de4d11a3c40381080caa44",
            "e049d5d20eed4a47b95332eef7ce5aac",
            "8036d4d584cd49ceb82bc75927b1595b",
            "9030cf78f79c4c76aadc28b4bc886633"
          ]
        },
        "outputId": "e2b696f3-e408-4346-e9e1-8e5d824d7b24"
      },
      "source": [
        "model_config = transformers.BertConfig.from_pretrained(config.MODEL_NAME)\n",
        "model_config.output_hidden_states = True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e581ced5946848e8a259b1d01e2d72ca",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hSK-7dWyNMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import gc\n",
        "\n",
        "def run(fold):\n",
        "    dfx = pd.read_csv(config.DATA_PATH)\n",
        "    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
        "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "    train_data = TwitterDataset(df_train.text.values, df_train.selected_text.values, \n",
        "                            df_train.sentiment.values, config.TOKENIZER, config.MAX_LEN)\n",
        "    \n",
        "    valid_data = TwitterDataset(df_valid.text.values, df_valid.selected_text.values, \n",
        "                            df_valid.sentiment.values, config.TOKENIZER, config.MAX_LEN)\n",
        "    \n",
        "    # make dataloaders\n",
        "    train_dataloader = DataLoader(train_data, batch_size=config.TRAIN_BATCH_SIZE, \n",
        "                                  shuffle=True, num_workers=0)\n",
        "    \n",
        "    valid_dataloader = DataLoader(valid_data, batch_size=config.VALID_BATCH_SIZE, \n",
        "                                  shuffle=True, num_workers=0)\n",
        "    \n",
        "    # model_config = transformers.BertConfig.from_pretrained(config.MODEL_NAME)\n",
        "    # model_config.output_hidden_states = True\n",
        "\n",
        "    # set the device\n",
        "    device = torch.device('cuda')\n",
        "    \n",
        "    gc.collect()\n",
        "    model = Bert(xtra_config=model_config).to(device)\n",
        "\n",
        "    # parmas you want optimized\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "\n",
        "    # we don't want weight decay for these\n",
        "    no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']\n",
        "\n",
        "    optimizer_params = [\n",
        "        {'params': [p for n, p in param_optimizer if n not in no_decay], \n",
        "         'weight_decay':0.001},\n",
        "        #  no weight decay should be applied\n",
        "        {'params': [p for n, p in param_optimizer if n in no_decay],\n",
        "         'weight_decay':0.0}\n",
        "    ]\n",
        "\n",
        "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.NUM_EPOCHS)\n",
        "    optimizer = AdamW(optimizer_params, lr=3e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer = optimizer,\n",
        "        num_training_steps = num_train_steps,\n",
        "        # no warmup\n",
        "        num_warmup_steps = 0 \n",
        "    )\n",
        "\n",
        "    es = EarlyStopping(patience=2, mode='max')\n",
        "    print(f'\\n Training starting for fold {fold}')\n",
        "\n",
        "    for epoch in range(config.NUM_EPOCHS):\n",
        "        gc.collect()\n",
        "        train_fn(train_dataloader, model, optimizer, scheduler, device)\n",
        "        gc.collect()\n",
        "        jaccard = eval_fn(valid_dataloader, model, device)\n",
        "        es(jaccard, model, model_path=model_path/f'finetunedmodel_{fold}.pth')\n",
        "        if es.early_stop:\n",
        "            print('Early Stopping')\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFGZAZwlG5xZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "799374f8-601e-432f-ce3e-209ea228c1aa"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "709"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56Eo65cVmG32",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "d3d77520bef449c39c862005a69375c9",
            "2536c3fdc49e4a08959fa7dc47018d91",
            "430c9c649552431181b319e6a002d1ee",
            "18ebad46b2c04d47900840b9e0adee27",
            "09b9870b166946548477a377c73be3a5",
            "c5eaa1a32bfd4ca7a0fd050dff666948"
          ]
        },
        "outputId": "8b3609ff-3bc6-4416-ba5e-26daf6cd6cf4"
      },
      "source": [
        "run(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Training starting for fold 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3d77520bef449c39c862005a69375c9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=2748), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09b9870b166946548477a377c73be3a5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1374), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Jaccard = 0.007563465352897634\n",
            "Validation score improved (-inf --> 0.007563465352897634). Saving model!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5eaa1a32bfd4ca7a0fd050dff666948",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=2748), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w70qRFbJnvgL",
        "colab_type": "text"
      },
      "source": [
        "##Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnbECr0ioxnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = pd.read_csv(data_path/'test.csv')\n",
        "# fill with dummy\n",
        "df_test['selected_test'] = df_test.text.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBEcPB1npC3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1umElD3pIF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model0 = Bert(xtra_config=model_config).to(device)\n",
        "model0.load_state_dict(torch.load(model_path/f'finetunedmodel_0.pth'))\n",
        "# put in eval mode for inference\n",
        "model0.eval()\n",
        "\n",
        "model1 = Bert(xtra_config=model_config).to(device)\n",
        "model1.load_state_dict(torch.load(model_path/f'finetunedmodel_1.pth'))\n",
        "# put in eval mode for inference\n",
        "model1.eval()\n",
        "\n",
        "model2 = Bert(xtra_config=model_config).to(device)\n",
        "model2.load_state_dict(torch.load(model_path/f'finetunedmodel_2.pth'))\n",
        "# put in eval mode for inference\n",
        "model2.eval()\n",
        "\n",
        "model3 = Bert(xtra_config=model_config).to(device)\n",
        "model3.load_state_dict(torch.load(model_path/f'finetunedmodel_3.pth'))\n",
        "# put in eval mode for inference\n",
        "model3.eval()\n",
        "\n",
        "model4 = Bert(xtra_config=model_config).to(device)\n",
        "model4.load_state_dict(torch.load(model_path/f'finetunedmodel_4.pth'))\n",
        "# put in eval mode for inference\n",
        "model4.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx0izFvBqEtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = TwitterDataset(df_test.text.values, df_test.selected_text.values, \n",
        "                            df_test.sentiment.values, config.TOKENIZER, config.MAX_LEN)\n",
        "    \n",
        "# make dataloaders\n",
        "test_dataloader = DataLoader(test_data, batch_size=config.TRAIN_BATCH_SIZE, \n",
        "                                shuffle=True, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ-6OOGprEF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    tk0 = notebook.tqdm(test_dataloader, total=len(test_dataloader))\n",
        "\n",
        "    for i, data in enumerate(tk0):\n",
        "        input_ids = data['input_ids']\n",
        "        token_ids = data['token_ids']\n",
        "        mask = data['mask']\n",
        "        target_start = data['target_start']\n",
        "        target_end = data['target_end']\n",
        "        tweet_offsets = data['tweet_offsets']\n",
        "        tweet = data['tweet']\n",
        "        extracted_text = data['extracted_text']\n",
        "        sentiment = data['sentiment']\n",
        "\n",
        "        # put em on the device\n",
        "        input_ids = input_ids.to(device).long()\n",
        "        token_ids = token_ids.to(device).long()\n",
        "        mask = mask.to(device).long()\n",
        "        target_start = target_start.to(device).long()\n",
        "        target_end = target_end.to(device).long()\n",
        "        tweet_offsets = tweet_offsets.to(device).long()\n",
        "\n",
        "        # push it into the model\n",
        "        pred_start0, pred_end0 = model0(\n",
        "            stoi = input_ids, \n",
        "            mask = mask, \n",
        "            token_type_ids = token_ids\n",
        "        )\n",
        "\n",
        "        pred_start1, pred_end1 = model1(\n",
        "            stoi = input_ids, \n",
        "            mask = mask, \n",
        "            token_type_ids = token_ids\n",
        "        )\n",
        "\n",
        "        pred_start2, pred_end2 = model2(\n",
        "            stoi = input_ids, \n",
        "            mask = mask, \n",
        "            token_type_ids = token_ids\n",
        "        )\n",
        "\n",
        "        pred_start3, pred_end3 = model3(\n",
        "            stoi = input_ids, \n",
        "            mask = mask, \n",
        "            token_type_ids = token_ids\n",
        "        )\n",
        "\n",
        "        pred_start4, pred_end4 = model4(\n",
        "            stoi = input_ids, \n",
        "            mask = mask, \n",
        "            token_type_ids = token_ids\n",
        "        )\n",
        "\n",
        "\n",
        "        # Ensemble by taking averages\n",
        "        avg_pred_start = (pred_start0, pred_start1, pred_start2, pred_start3, pred_start4) / 5\n",
        "        avg_pred_end = (pred_end0, pred_end1, pred_end2, pred_end3, pred_end4) / 5\n",
        "\n",
        "        # we have to convert the predicted logits to probability values for each targ\n",
        "        pred_start_probs = torch.softmax(avg_pred_start, dim=1).cpu().detach().numpy()\n",
        "        pred_end_probs = torch.softmax(avg_pred_end, dim=1).cpu().detach().numpy()\n",
        "\n",
        "        for idx, tweet_ in enumerate(tweet):\n",
        "            pred_start_prob = np.argmax(pred_start_probs[idx, :])\n",
        "            pred_end_prob = np.argmax(pred_end_probs[idx, :])\n",
        "\n",
        "            _, pred_extract = calculate_jaccard(tweet_, extracted_text[idx], sentiment[idx], \n",
        "                                tweet_offsets[idx], pred_start_prob, pred_end_prob)\n",
        "            \n",
        "            prediction.append(pred_extract)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Miko2Xb3uBQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def post_process(selected):\n",
        "    return \" \".join(set(selected.lower().split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUfmnXuAtmhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = pd.read_csv(data_path/\"sample_submission.csv\")\n",
        "sample.loc[:, 'selected_text'] = final_output\n",
        "sample.selected_text = sample.selected_text.map(post_process)\n",
        "sample.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN8twOVr9ZhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}