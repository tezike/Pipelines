{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Raw_Bert_IMDB.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "StWgJac61ouP",
        "U5QpUYia16Db",
        "rLWTDjr86-L3",
        "baMjDbCW2ZoO",
        "qs1T6cjcCYNl"
      ],
      "authorship_tag": "ABX9TyM/nwoZ7gco3HDXiZ9qBxbf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18705c1554304ff9a2734db6c708570e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f174f6b66a3a4475a4525bac43028fd7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_93f50af554af4c9e948e1c00463608eb",
              "IPY_MODEL_7065a1e2a0d2405ea2c20d643926d72f"
            ]
          }
        },
        "f174f6b66a3a4475a4525bac43028fd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "93f50af554af4c9e948e1c00463608eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b722500da0454dd4ace8dca7187b0f9c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_103ddc253933425fa289d54a9d7c9506"
          }
        },
        "7065a1e2a0d2405ea2c20d643926d72f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9253d547b46f4624a68087e4529d836e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.12MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_36957a0a06664dc8919a28dbd0d4dd77"
          }
        },
        "b722500da0454dd4ace8dca7187b0f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "103ddc253933425fa289d54a9d7c9506": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9253d547b46f4624a68087e4529d836e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "36957a0a06664dc8919a28dbd0d4dd77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQNK70EuxrRr",
        "colab_type": "text"
      },
      "source": [
        "## Colab_setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiTyipOvcPQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nipZBIQ2amuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_path(path):\n",
        "    if not os.path.isdir(path):\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "    return path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdKS3nNT0IKp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "8e0e35bd-00bc-4f1d-99ad-3c217b726228"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root_dir = Path('/content/drive/My Drive')\n",
        "base_path = create_path(root_dir/'Bert')\n",
        "base_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/My Drive/Bert')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIXgl6523wqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colab_path = Path('/content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BTdMY8caahd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = create_path(base_path/'dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw59N97DV63P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = create_path(base_path/'models')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIp4JAvv3rGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_path = (create_path(colab_path/'input/bert_uncased'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcRo2fCS1a49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############join all above to below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usjQeC0LGK1A",
        "colab_type": "text"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSj-x_eGJnTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"'https://storage.googleapis.com/kaggle-data-sets/134715/320111/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587033973&Signature=ldODSCHFe%2FEkLQ1K%2F7Jp8zS3%2B8C4WwUXdtaUk0rQv8sS%2BSvNjLynwk1%2FHqQgPbXc8VxIfZUTa%2F5ZnuE2sdqa0jsBmfvoQMII%2Fg6RRkvzx0APSFeajiVEnWf5dMMZTb1JDRKvM6DM4900brshBalN0%2BiwsXmdngokJ9FHQiNvcZKHlVhsUtqQeHidYDqyUVlXgSBCT6ZEtdGhJLSAEvHqSNabRsXR5VjiMpJqAb26HCm1R%2F7%2FIKpXUyJzF5BmxW%2BhZoydukE5QHTjXjlwbEdfHTjKooX2lNq13Z%2BCQCEeC8b3pwbPSEnyGwnZiZwEVoKCQnzF2LrFnB3CubBXDE5qbA%3D%3D&response-content-disposition=attachment%3B+filename%3Dimdb-dataset-of-50k-movie-reviews.zip'\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7NoYjpfGN_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(data_path)\n",
        "!wget -q {str(url)} -O temp.zip && unzip -q temp.zip && rm 'temp.zip'\n",
        "os.chdir(colab_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StWgJac61ouP",
        "colab_type": "text"
      },
      "source": [
        "## config.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRhI9sD7LKGq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "fd7c393e-fef5-40c0-db3a-baf4c6c7ab3c"
      },
      "source": [
        "!pip -q install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 645kB 17.5MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 50.0MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0MB 47.2MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 40.0MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLfoy4Kt1cSo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "18705c1554304ff9a2734db6c708570e",
            "f174f6b66a3a4475a4525bac43028fd7",
            "93f50af554af4c9e948e1c00463608eb",
            "7065a1e2a0d2405ea2c20d643926d72f",
            "b722500da0454dd4ace8dca7187b0f9c",
            "103ddc253933425fa289d54a9d7c9506",
            "9253d547b46f4624a68087e4529d836e",
            "36957a0a06664dc8919a28dbd0d4dd77"
          ]
        },
        "outputId": "ba43e878-500c-4ad5-86e1-dadea27bc817"
      },
      "source": [
        "import transformers\n",
        "class Config():\n",
        "    def __init__(self):\n",
        "        self.MAX_LEN = 512\n",
        "        self.SAVE_MODEL_PATH = str(model_path/'finetuned-bert.pth')\n",
        "        self.DATA_PATH = str(data_path/'IMDB Dataset.csv')\n",
        "        self.BERT_PATH = str(bert_path/'finetuned-bert-2.pth')\n",
        "        self.TRAIN_BATCH_SIZE = 8\n",
        "        self.VALID_BATCH_SIZE = 4\n",
        "        self.NUM_EPOCHS = 10\n",
        "        self.MODEL_NAME = 'bert-base-uncased'\n",
        "        self.TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
        "            pretrained_model_name_or_path=self.MODEL_NAME,\n",
        "            do_lower_case=True,\n",
        "            force_download = True,\n",
        "        )\n",
        "\n",
        "config = Config()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18705c1554304ff9a2734db6c708570e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_widâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2uWu1-Z1yTg",
        "colab_type": "text"
      },
      "source": [
        "##model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4zlUSBu2vXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import transformers\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# make a pythorch model\n",
        "class Bert(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Bert, self).__init__()\n",
        "        # load a pretrained bert model arch\n",
        "        self.bert = transformers.BertModel.from_pretrained(config.MODEL_NAME)\n",
        "        #  dropout should be applied\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        # a classifier head should be placed\n",
        "        self.head = nn.Linear(768, 1)\n",
        "\n",
        "        # by default sigmoid will be placed after this head\n",
        "\n",
        "    def forward(self, stoi, mask, token_type_ids):\n",
        "        final_hidden, output = self.bert(input_ids=stoi, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "        # pass into dropout\n",
        "        output = self.drop(output)\n",
        "        # pass into classifier head\n",
        "        output = self.head(output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5QpUYia16Db",
        "colab_type": "text"
      },
      "source": [
        "## data.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scF2iZf67Bhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, text, targ):\n",
        "        self.text = text\n",
        "        self.targ = targ\n",
        "\n",
        "        # use bert default tokenizer\n",
        "        self.tokenizer = config.TOKENIZER\n",
        "        self.max_len = config.MAX_LEN\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sanity check\n",
        "        text = str(self.text[idx])\n",
        "        text = ' '.join(text.split())\n",
        "        \n",
        "        inputs = self.tokenizer.encode_plus(text=text, text_pair=None,\n",
        "                                            add_special_tokens=True, \n",
        "                                            max_length=self.max_len, \n",
        "                                            pad_to_max_length=True, \n",
        "                                            padding_side='right'\n",
        "                                            )\n",
        "        \n",
        "        # input_ids is the integer repr of every token\n",
        "        stoi = inputs['input_ids']\n",
        "        # the attention_mask is the integer repr of the parts of the text to be attended to by the model. \n",
        "        # 1 means attend and 0 means not. 0 usually covers all the padding\n",
        "        mask = inputs['attention_mask']\n",
        "        # used to show question and answer in question answering pair where question is repr with 0 and answer with 1\n",
        "        token_type_ids = inputs['token_type_ids']\n",
        "\n",
        "        # # # use zero-right padding for the `inputs` keys\n",
        "        # if len(stoi) <= self.max_len:\n",
        "        #     padding_size = self.max_len - len(stoi)\n",
        "        #     stoi = stoi + ([0]*padding_size)\n",
        "        #     mask = mask + ([0]*padding_size)\n",
        "        #     token_type_ids = token_type_ids + ([0]*padding_size)\n",
        "        # else:\n",
        "        #     stoi = stoi[:self.max_len]\n",
        "        #     mask = mask[:self.max_len]\n",
        "        #     token_type_ids = token_type_ids[:self.max_len]\n",
        "        \n",
        "        return {\n",
        "            'stoi': torch.tensor(stoi).long(),\n",
        "            'mask': torch.tensor(mask).long(),\n",
        "            'token_type_ids': torch.tensor(token_type_ids).long(),\n",
        "            'target': torch.tensor(self.targ[idx])\n",
        "                }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzRAEuy7D92j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a488bc1-7fc5-4dd1-c6c3-e5ad63dceeb2"
      },
      "source": [
        "example_text = 'The sheep jumped over the fence'\n",
        "config.TOKENIZER.encode(text=example_text, add_special_tokens=True, max_lenght=512, )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101, 1996, 8351, 5598, 2058, 1996, 8638, 102]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-_7R7WvOEGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d41f6226-8394-41c6-a7bd-91f21429ac71"
      },
      "source": [
        "config.TOKENIZER.decode([101, 1996, 8351, 5598, 2058, 1996, 8638, 102])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] the sheep jumped over the fence [SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKMdcIICOOFW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "213bc214-4471-432b-a049-e5bc7200007e"
      },
      "source": [
        "config.TOKENIZER.convert_tokens_to_ids('[CLS] the sheep jumped over the fence [SEP]'.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101, 1996, 8351, 5598, 2058, 1996, 8638, 102]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM_gkDXUFwWk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99fb4daf-4ae6-4323-9014-e1a36f700da1"
      },
      "source": [
        "# itos\n",
        "config.TOKENIZER.convert_ids_to_tokens([101, 1996, 8351, 5598, 2058, 1996, 8638, 102])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', 'the', 'sheep', 'jumped', 'over', 'the', 'fence', '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rLWTDjr86-L3"
      },
      "source": [
        "## train_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IejDf-D2wKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def training(data_loader, model, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    for i, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "        ids = data['stoi']\n",
        "        mask = data['mask']\n",
        "        token_type_ids = data['token_type_ids']\n",
        "        target = data['target']\n",
        "\n",
        "        #put on device\n",
        "        ids = ids.to(device).long()\n",
        "        mask = mask.to(device).long()\n",
        "        token_type_ids = token_type_ids.to(device).long()\n",
        "        target = target.to(device).float()\n",
        "\n",
        "        #clear optimizer grads\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(\n",
        "            stoi = ids,\n",
        "            mask = mask,\n",
        "            token_type_ids = token_type_ids\n",
        "        )\n",
        "\n",
        "        loss = loss_fn(output, target.view(-1, 1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    \n",
        "\n",
        "def evaluate(data_loader, model, optimizer, scheduler, device):\n",
        "    model.eval()\n",
        "    # track the targets and the outputs\n",
        "    last_output, last_target = [], []\n",
        "\n",
        "    # when doing evaluation, it is important to remember to not track the gradients\n",
        "    with torch.no_grad():\n",
        "        for i, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "            ids = data['stoi']\n",
        "            mask = data['mask']\n",
        "            token_type_ids = data['token_type_ids']\n",
        "            target = data['target']\n",
        "\n",
        "            #put on device\n",
        "            ids = ids.to(device).long()\n",
        "            mask = mask.to(device).long()\n",
        "            token_type_ids = token_type_ids.to(device).long()\n",
        "            target = target.to(device).float()\n",
        "\n",
        "            output = model(\n",
        "                stoi = ids,\n",
        "                mask = mask,\n",
        "                token_type_ids = token_type_ids\n",
        "            )\n",
        "\n",
        "            # detach and convert to arrays\n",
        "            last_target.extend(target.cpu().detach().numpy())\n",
        "            last_output.extend(output.cpu().detach().numpy())\n",
        "\n",
        "    return last_target, last_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baMjDbCW2ZoO",
        "colab_type": "text"
      },
      "source": [
        "##train.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N8ogCkECamZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "df = pd.read_csv(config.DATA_PATH)\n",
        "df = df[['review', 'sentiment']]\n",
        "\n",
        "le.fit(df.sentiment.values)\n",
        "df.sentiment = le.transform(df.sentiment.values)\n",
        "\n",
        "# split it\n",
        "train, valid = train_test_split(df, test_size= 0.2, random_state=42, \n",
        "                                stratify=df.sentiment.values)\n",
        "\n",
        "# reset the index in the dfs\n",
        "train.reset_index(inplace=True, drop=True)\n",
        "valid.reset_index(inplace=True, drop=True)\n",
        "\n",
        "# make train and valid dataloaders\n",
        "train_dataset = BertDataset(train.review.values, train.sentiment.values)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.TRAIN_BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "\n",
        "valid_dataset = BertDataset(valid.review.values, valid.sentiment.values)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=config.VALID_BATCH_SIZE, shuffle=True, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rttoaWhnPYn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the device\n",
        "device = torch.device('cuda')\n",
        "\n",
        "model = Bert().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCC6_Pb4Pmqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parmas you want optimized\n",
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "# we don't want weight decay for these\n",
        "no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnEC9gTeQ7Ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer_params = [\n",
        "        {'params': [p for n, p in param_optimizer if n not in no_decay], \n",
        "         'weight_decay':0.001},\n",
        "        #  no weight decay should be applied\n",
        "        {'params': [p for n, p in param_optimizer if n in no_decay],\n",
        "         'weight_decay':0.0}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh6CY4SdXER3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "num_train_steps = int(len(train) / config.TRAIN_BATCH_SIZE * config.NUM_EPOCHS)\n",
        "optimizer = AdamW(optimizer_params, lr=3e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer = optimizer,\n",
        "    num_training_steps = num_train_steps,\n",
        "    # no warmup\n",
        "    num_warmup_steps = 0 \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYOT3Cg_3UrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d8e504f-cc32-4804-ffc4-9b0d61dbd802"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "231"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x7YvpetYZId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "216b647d-aa0d-46c4-ee42-b7667df80f47"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "best_accuracy = 0\n",
        "for epoch in range(config.NUM_EPOCHS):\n",
        "    # train\n",
        "    training(train_dataloader, model, optimizer, scheduler, device)\n",
        "\n",
        "    # eval\n",
        "    target, output = evaluate(valid_dataloader, model, optimizer, scheduler, device)\n",
        "\n",
        "    # we have to check if the output gotten is greater than o.5 or not. \n",
        "    # Because we use sigmoid in model final layer\n",
        "    output =  np.array(output) >= 0.5 #returns bool\n",
        "\n",
        "    # calculate accuracy\n",
        "    accuracy =  accuracy_score(target, output)\n",
        "    print(f'\\n Accuracy Score: {accuracy}')\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(model.state_dict(), config.SAVE_MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [39:26<00:00,  2.11it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [03:45<00:00, 11.08it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Accuracy Score: 0.9395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 23%|â–ˆâ–ˆâ–Ž       | 1131/5000 [08:57<30:59,  2.08it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmmO9ZRJKi29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predictor(sentence, model, device=device):\n",
        "    # instantiate tokenizer\n",
        "    tokenizer = config.TOKENIZER\n",
        "    max_len = config.MAX_LEN\n",
        "\n",
        "    inputs = tokenizer.encode_plus(text=sentence[:max_len], text_pair=None,\n",
        "                                        add_special_tokens=True, \n",
        "                                        max_lenght=max_len, \n",
        "                                        pad_to_max_lenght=True\n",
        "                                        )\n",
        "    \n",
        "    stoi = inputs['input_ids']\n",
        "    mask = inputs['attention_mask']\n",
        "    token_type_ids = inputs['token_type_ids']\n",
        "\n",
        "    # # use zero-right padding for the `inputs` keys\n",
        "    if len(stoi) <= max_len:\n",
        "        padding_size = max_len - len(stoi)\n",
        "        stoi = stoi + ([0]*padding_size)\n",
        "        mask = mask + ([0]*padding_size)\n",
        "        token_type_ids = token_type_ids + ([0]*padding_size)\n",
        "    else:\n",
        "        stoi = stoi[:max_len]\n",
        "        mask = mask[:max_len]\n",
        "        token_type_ids = token_type_ids[:max_len]\n",
        "    \n",
        "    # dont' forget to add an extra batch\n",
        "    stoi =  torch.tensor(stoi).long().unsqueeze(0)\n",
        "    mask = torch.tensor(mask).long().unsqueeze(0)\n",
        "    token_type_ids =  torch.tensor(token_type_ids).long().unsqueeze(0)\n",
        "\n",
        "    ids = stoi.to(device).long()\n",
        "    mask = mask.to(device).long()\n",
        "    token_type_ids = token_type_ids.to(device).long()\n",
        "\n",
        "    output = model(\n",
        "            stoi = ids,\n",
        "            mask = mask,\n",
        "            token_type_ids = token_type_ids\n",
        "        )\n",
        "    \n",
        "    # limit the result to within 0 and 1 using sigmoid\n",
        "    output = torch.sigmoid(output).cpu().detach().numpy()\n",
        "    print(output)\n",
        "    return output[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3riM9ChQKqzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL = Bert().to(device)\n",
        "MODEL.load_state_dict(torch.load(config.SAVE_MODEL_PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAZZ6gW8Ln2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = 'This is a bad movie'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAHUaS_QLaBZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "682d7027-e02e-404d-b9cb-718567344197"
      },
      "source": [
        "pos_pred = predictor(sentence, model=MODEL, device=device)\n",
        "neg_pred = 1-pos_pred\n",
        "response = dict()\n",
        "response['response'] = {\n",
        "    'Poistive: ': str(pos_pred),\n",
        "    'Negative: ': str(neg_pred),\n",
        "    'Sentence:': str(sentence)\n",
        "}\n",
        "response"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.6289537]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'response': {'Negative: ': '0.3710463047027588',\n",
              "  'Poistive: ': '0.6289537',\n",
              "  'Sentence:': 'This is a bad movie'}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qs1T6cjcCYNl"
      },
      "source": [
        "##app.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYmGNglw2wuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " import flask\n",
        " from flask import Flask, request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD-RK_da3Rv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize app\n",
        "app = Flask()\n",
        "\n",
        "MODEL = None\n",
        "device = 'cpu'\n",
        "\n",
        "def predictor(sentence, model, device=device):\n",
        "    # instantiate tokenizer\n",
        "    tokenizer = config.TOKENIZER\n",
        "    max_len = config.MAX_LEN\n",
        "\n",
        "    inputs = tokenizer.encode_plus(text=sentence[:max_len], text_pair=None,\n",
        "                                        add_special_tokens=True, \n",
        "                                        max_lenght=max_len, \n",
        "                                        pad_to_max_lenght=True\n",
        "                                        )\n",
        "    \n",
        "    stoi = inputs['input_ids']\n",
        "    mask = inputs['attention_mask']\n",
        "    token_type_ids = inputs['token_type_ids']\n",
        "\n",
        "    # # use zero-right padding for the `inputs` keys\n",
        "    if len(stoi) <= max_len:\n",
        "        padding_size = max_len - len(stoi)\n",
        "        stoi = stoi + ([0]*padding_size)\n",
        "        mask = mask + ([0]*padding_size)\n",
        "        token_type_ids = token_type_ids + ([0]*padding_size)\n",
        "    else:\n",
        "        stoi = stoi[:max_len]\n",
        "        mask = mask[:max_len]\n",
        "        token_type_ids = token_type_ids[:max_len]\n",
        "    \n",
        "    # dont' forget to add an extra batch\n",
        "    stoi =  torch.tensor(stoi).long().unsqueeze(0)\n",
        "    mask = torch.tensor(mask).long().unsqueeze(0)\n",
        "    token_type_ids =  torch.tensor(token_type_ids).long().unsqueeze(0)\n",
        "\n",
        "    ids = ids.to(device).long()\n",
        "    mask = mask.to(device).long()\n",
        "    token_type_ids = token_type_ids.to(device).long()\n",
        "\n",
        "    output = model(\n",
        "            stoi = ids,\n",
        "            mask = mask,\n",
        "            token_type_ids = token_type_ids\n",
        "        )\n",
        "    \n",
        "    # limit the result to within 0 and 1 using sigmoid\n",
        "    output = torch.sigmoid(output).numpy()\n",
        "    print(output)\n",
        "    return output[0][0]\n",
        "\n",
        "@app.route('/predict')\n",
        "def predict():\n",
        "     sentence = request.args.get('sentence')\n",
        "     pos_pred = predictor(sentence, model=model, device=device)\n",
        "     neg_pred = 1-pos_pred\n",
        "     response = dict()\n",
        "     response['response'] = {\n",
        "         'Poistive: ': str(pos_pred),\n",
        "         'Negative: ': str(neg_pred),\n",
        "         'Sentence:': str(sentence)\n",
        "     }\n",
        "     return response\n",
        " \n",
        "if __name__ == '__main__':\n",
        "    MODEL = BERT().to(device)\n",
        "    MODEL.eval()\n",
        "\n",
        "    # load the model state dict\n",
        "    MODEL = torch.load_state_dict(torch.load(config.SAVE_MODEL_PATH))\n",
        "    app.run(debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}